# Week 6: n-step Methods & Eligibility Traces

## Learning Objectives

By the end of this week, you should be able to:

1. Understand **n-step returns** as a bridge between Monte Carlo and TD methods
2. Implement **n-step TD** for prediction and **n-step SARSA** for control
3. Understand the **Œª-return** and **TD(Œª)** algorithm
4. Distinguish between **forward view** and **backward view** of TD(Œª)
5. Implement **eligibility traces** for efficient credit assignment
6. Compare **accumulating**, **replacing**, and **dutch** traces
7. Apply eligibility traces to improve learning speed

## Key Concepts

### 1. n-step Returns - The TD-MC Spectrum

**The Unifying Idea**: We can interpolate between TD (n=1) and Monte Carlo (n=‚àû) by looking n steps ahead.

**n-step Return Definition**:
```
G_t:t+n = R_{t+1} + Œ≥R_{t+2} + Œ≥¬≤R_{t+3} + ... + Œ≥^{n-1}R_{t+n} + Œ≥^n V(S_{t+n})
        = Œ£_{k=0}^{n-1} Œ≥^k R_{t+k+1} + Œ≥^n V(S_{t+n})
```

**Special Cases**:
- **n=1** (TD(0)): G_t:t+1 = R_{t+1} + Œ≥V(S_{t+1})
- **n=‚àû** (Monte Carlo): G_t:t+‚àû = R_{t+1} + Œ≥R_{t+2} + Œ≥¬≤R_{t+3} + ... = G_t

**n-step TD Update**:
```
V(S_t) ‚Üê V(S_t) + Œ±[G_t:t+n - V(S_t)]
```

**Properties**:
- Bias decreases as n increases (more actual rewards, less bootstrapping)
- Variance increases as n increases (more random rewards in return)
- Optimal n depends on task: typically n ‚àà [3, 10] works well

### 2. n-step SARSA - Control with n-step Returns

Extend n-step returns to action values for control.

**n-step Q-return**:
```
G_t:t+n = R_{t+1} + Œ≥R_{t+2} + ... + Œ≥^{n-1}R_{t+n} + Œ≥^n Q(S_{t+n}, A_{t+n})
```

**n-step SARSA Update**:
```
Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±[G_t:t+n - Q(S_t, A_t)]
```

**Algorithm Requirements**:
- Must store last n state-action pairs: (S_t, A_t), ..., (S_{t+n-1}, A_{t+n-1})
- Updates are delayed by n steps
- At episode end, must handle truncated returns

**n-step Off-Policy Variants**:
- **n-step Q-Learning**: Use max_a Q(S_{t+n}, a) instead of Q(S_{t+n}, A_{t+n})
- **n-step Tree Backup**: Incorporate importance sampling for off-policy learning

### 3. TD(Œª) - The Œª-Return

Instead of choosing a single n, **average over all n-step returns** weighted by Œª.

**Œª-return Definition**:
```
G_t^Œª = (1-Œª) Œ£_{n=1}^{‚àû} Œª^{n-1} G_t:t+n
```

This is a **weighted average** of all n-step returns:
- More weight on shorter returns (small n) when Œª is small
- More weight on longer returns (large n) when Œª is large

**Special Cases**:
- **Œª=0**: G_t^Œª = G_t:t+1 (TD(0), one-step return)
- **Œª=1**: G_t^Œª = G_t (Monte Carlo, complete return)

**Expansion**:
```
G_t^Œª = (1-Œª)[G_t:t+1 + ŒªG_t:t+2 + Œª¬≤G_t:t+3 + ... ] + Œª^{T-t-1}G_t
      = (1-Œª) Œ£_{n=1}^{T-t-1} Œª^{n-1} G_t:t+n + Œª^{T-t-1} G_t
```

**TD(Œª) Update (Forward View)**:
```
V(S_t) ‚Üê V(S_t) + Œ±[G_t^Œª - V(S_t)]
```

**Problem**: This is not computable online! Need the entire episode to compute G_t^Œª.

**Solution**: Backward view with eligibility traces (equivalent but online).

### 4. Forward View vs Backward View

**Forward View** (conceptual):
- Look forward from current state
- Weight future n-step returns by Œª
- Requires complete trajectory (offline)
- Useful for understanding

**Backward View** (implementable):
- Look backward from current TD error
- Credit assignment through eligibility traces
- Fully online (update at each step)
- Practical algorithm

**Theorem (Equivalence)**:
Under certain conditions, forward and backward views produce identical updates when summed over an episode.

### 5. Eligibility Traces - Efficient Credit Assignment

**The Problem**: When reward is received, which past states/actions deserve credit?

**Solution**: Maintain an **eligibility trace** e_t(s) for each state s.

**Trace Update (Accumulating)**:
```
e_t(s) = {
  Œ≥Œª e_{t-1}(s) + 1   if s = S_t
  Œ≥Œª e_{t-1}(s)       otherwise
}
```

**Interpretation**:
- e_t(s) tracks how "eligible" state s is for receiving credit
- Increases by 1 when state is visited
- Decays by Œ≥Œª at each step

**TD(Œª) Update with Traces (Backward View)**:
```
Œ¥_t = R_{t+1} + Œ≥V(S_{t+1}) - V(S_t)   (TD error)

For all s ‚àà S:
    e_t(s) ‚Üê Œ≥Œª e_{t-1}(s) + ùüô(s = S_t)
    V(s) ‚Üê V(s) + Œ± Œ¥_t e_t(s)
```

**Key Insight**: All states are updated at each step, weighted by their eligibility trace!

**Three Types of Traces**:

1. **Accumulating Traces**:
   ```
   e_t(s) ‚Üê Œ≥Œª e_{t-1}(s) + ùüô(s = S_t)
   ```
   - Traces accumulate with repeated visits
   - Standard choice

2. **Replacing Traces**:
   ```
   e_t(s) ‚Üê max(Œ≥Œª e_{t-1}(s), ùüô(s = S_t))
   ```
   - Reset to 1 on visit (don't accumulate)
   - Often works better in practice
   - Particularly good with function approximation

3. **Dutch Traces**:
   ```
   e_t(s) ‚Üê (1 - Œ±) Œ≥Œª e_{t-1}(s) + ùüô(s = S_t)
   ```
   - Accounts for learning rate in trace decay
   - Theoretical advantages
   - Less common in practice

### 6. SARSA(Œª) - Control with Eligibility Traces

Extend TD(Œª) to action values for control.

**SARSA(Œª) Algorithm**:
```
Initialize Q(s,a) arbitrarily, e(s,a) = 0 for all s,a

For each episode:
    Initialize S, choose A using policy from Q (Œµ-greedy)
    e(s,a) = 0 for all s,a

    For each step:
        Take action A, observe R, S'
        Choose A' from S' using policy from Q
        Œ¥ ‚Üê R + Œ≥Q(S',A') - Q(S,A)

        e(S,A) ‚Üê e(S,A) + 1  (or replace: e(S,A) ‚Üê 1)

        For all s,a:
            Q(s,a) ‚Üê Q(s,a) + Œ± Œ¥ e(s,a)
            e(s,a) ‚Üê Œ≥Œª e(s,a)

        S ‚Üê S', A ‚Üê A'
    Until S is terminal
```

**Advantages**:
- Faster learning than SARSA (credit spreads backward)
- Single parameter Œª controls credit assignment
- Particularly effective for sparse rewards

## Key Equations

### n-step Return
```
G_t:t+n = Œ£_{k=0}^{n-1} Œ≥^k R_{t+k+1} + Œ≥^n V(S_{t+n})

n-step TD: V(S_t) ‚Üê V(S_t) + Œ±[G_t:t+n - V(S_t)]
```

### Œª-return
```
G_t^Œª = (1-Œª) Œ£_{n=1}^{‚àû} Œª^{n-1} G_t:t+n

Special cases:
  Œª=0: G_t^Œª = G_t:t+1  (TD(0))
  Œª=1: G_t^Œª = G_t      (MC)
```

### TD(Œª) with Eligibility Traces (Tabular)
```
Œ¥_t = R_{t+1} + Œ≥V(S_{t+1}) - V(S_t)

e_t(s) = Œ≥Œª e_{t-1}(s) + ùüô(s = S_t)

V(s) ‚Üê V(s) + Œ± Œ¥_t e_t(s)  for all s
```

### SARSA(Œª)
```
Œ¥_t = R_{t+1} + Œ≥Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)

e_t(s,a) = Œ≥Œª e_{t-1}(s,a) + ùüô(s = S_t, a = A_t)

Q(s,a) ‚Üê Q(s,a) + Œ± Œ¥_t e_t(s,a)  for all s,a
```

## Textbook References

- **Sutton & Barto**:
  - Chapter 7: n-step Bootstrapping
    - Section 7.1: n-step TD Prediction
    - Section 7.2: n-step SARSA
    - Section 7.3: n-step Off-policy Learning
  - Chapter 12: Eligibility Traces
    - Section 12.1: The Œª-return
    - Section 12.2: TD(Œª)
    - Section 12.3: n-step Truncated Œª-return Methods
    - Section 12.4: Redoing Updates: Online Œª-return Algorithm
    - Section 12.5: True Online TD(Œª)
    - Section 12.7: SARSA(Œª)
    - Section 12.10: Implementation Issues

- **David Silver's RL Course**:
  - Lecture 4: Model-Free Prediction (second half on eligibility traces)
  - [Lecture Slides](https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf)

- **CS234 Supplementary Material**:
  - Week 6: Multi-step TD and Eligibility Traces

## Implementation Tasks

### Task 1: 19-State Random Walk with n-step TD

The classic environment for comparing n-step methods (S&B Example 7.1).

**Environment**:
- States: 1, 2, 3, ..., 19 (start at 10)
- Terminal states: 0 (left, reward 0), 20 (right, reward 1)
- Random walk: equal probability left/right each step
- Discount: Œ≥ = 1

**True Values**: v_œÄ(i) = i/20 for i = 1, ..., 19

**Implementation**:
1. Implement n-step TD for n = 1, 2, 4, 8, 16, 32
2. Measure RMSE vs episodes for each n
3. Compare learning curves
4. Find optimal n

**Expected Observations**:
- n=1 (TD(0)): Slower convergence, smooth learning
- n=4 to 8: Typically best performance
- Large n (>16): Approaches MC, higher variance
- Optimal n depends on Œ±

### Task 2: Mountain Car with SARSA(Œª)

Apply eligibility traces to the challenging Mountain Car task.

**Environment**:
- Continuous state (position, velocity)
- Must discretize or use tile coding
- Sparse reward: -1 per step, 0 at goal
- Challenge: Must build momentum

**Implementation**:
1. Implement tile coding or discretization
2. Implement SARSA(Œª) with Œª = 0, 0.5, 0.9, 0.95
3. Compare learning speed (episodes to goal)
4. Visualize value function and policy

**Expected Observations**:
- Œª=0 (SARSA): Very slow, struggles with sparse reward
- Œª=0.9: Much faster, credit propagates backward
- Œª close to 1: Best performance for this sparse reward task

### Task 3: GridWorld with Replacing vs Accumulating Traces

Compare different trace types on a simple grid world.

**Environment**:
- 10√ó10 grid
- Start: bottom-left, Goal: top-right
- Obstacles scattered throughout
- Reward: -1 per step, 0 at goal

**Implementation**:
1. Implement SARSA(Œª) with accumulating traces
2. Implement SARSA(Œª) with replacing traces
3. Compare on grids with/without revisiting states
4. Measure episodes to convergence

**Expected Observations**:
- Similar performance on most tasks
- Replacing traces may converge faster with revisiting
- Accumulating traces more sensitive to Œª

### Task 4: Comparing n-step SARSA Variants

Implement and compare different n-step control methods.

**Methods to Compare**:
1. SARSA (n=1)
2. n-step SARSA (n=5)
3. n-step Expected SARSA
4. SARSA(Œª) with equivalent Œª

**Environments**: CliffWalking, Taxi, FrozenLake

**Analysis**:
- Learning curves (cumulative reward vs episodes)
- Sample efficiency
- Computational cost per step
- Final policy quality

## Comparison Tables

### n-step Methods Spectrum

| n | Method | Bias | Variance | Update Delay | Equivalent Œª |
|---|--------|------|----------|--------------|--------------|
| 1 | TD(0) | High | Low | 1 step | 0 |
| 2-10 | n-step TD | Medium | Medium | n steps | Varies |
| ‚àû | Monte Carlo | None | High | Episode end | 1 |

### Eligibility Traces Types

| Type | Update Rule | Behavior | Use Case |
|------|-------------|----------|----------|
| Accumulating | e ‚Üê Œ≥Œªe + 1 | Increases with visits | Standard choice |
| Replacing | e ‚Üê max(Œ≥Œªe, 1) | Resets to 1 | Function approximation |
| Dutch | e ‚Üê (1-Œ±)Œ≥Œªe + 1 | Learning-rate aware | Theoretical work |

### TD(Œª) Parameter Settings

| Œª | Behavior | Bias | Variance | Speed | Best For |
|---|----------|------|----------|-------|----------|
| 0 | TD(0) | High | Low | Fast | Short-term credit |
| 0.3-0.5 | Light traces | Medium | Low | Fast | General purpose |
| 0.8-0.9 | Medium traces | Low | Medium | Medium | Moderate delay |
| 0.95-0.99 | Heavy traces | Very low | High | Slow | Long delays, sparse rewards |
| 1.0 | MC | None | High | Slow | Episodic with accurate returns |

## Advantages of n-step and Eligibility Traces

**n-step Methods**:
1. **Tunable bias-variance**: Choose n to match task characteristics
2. **Intermediate convergence**: Often faster than both TD and MC
3. **Flexible**: Easy to understand and implement
4. **Effective**: n ‚àà [3, 10] works well for many tasks

**Eligibility Traces**:
1. **Efficient credit assignment**: Update all visited states, not just recent ones
2. **Single parameter Œª**: Easier to tune than choosing n
3. **Online learning**: No delay, update at every step
4. **Memory efficient**: Only store traces, not state history
5. **Fast learning**: Particularly for sparse rewards
6. **Bridges TD and MC**: Smoothly interpolates between extremes

## Practical Considerations

### Choosing n for n-step Methods

**Guidelines**:
- **Short episodes**: Use larger n (or MC)
- **Long episodes**: Use small n (3-10)
- **High variance**: Use smaller n
- **High bias**: Use larger n
- **Sparse rewards**: Use larger n or eligibility traces

**Tuning Strategy**:
1. Start with n=1 (TD) as baseline
2. Try n=4, 8, 16
3. Measure RMSE or learning speed
4. Choose best n for your task

### Choosing Œª for Eligibility Traces

**Guidelines**:
- **Dense rewards**: Œª = 0.3 to 0.7
- **Sparse rewards**: Œª = 0.9 to 0.99
- **Short-term dependencies**: Œª < 0.5
- **Long-term dependencies**: Œª > 0.8
- **Function approximation**: Often Œª = 0.9 works well

**Tuning Strategy**:
1. Start with Œª=0.5
2. If learning is slow, increase Œª (more credit to past states)
3. If learning is noisy, decrease Œª (less credit propagation)
4. Grid search over {0, 0.3, 0.5, 0.7, 0.9, 0.95}

### Implementation Tips

**1. Trace Initialization**:
```python
# Reset traces at episode start (episodic tasks)
e = np.zeros_like(Q)

# Or decay traces across episodes (continuing tasks)
e = gamma * lambda * e
```

**2. Efficient Trace Storage**:
```python
# Sparse traces (only store non-zero)
e = {}  # dict: (s,a) -> trace value

# Prune small traces
e = {k: v for k, v in e.items() if v > threshold}
```

**3. Learning Rate with Traces**:
- Eligibility traces amplify updates
- May need smaller Œ± than without traces
- Try Œ± = 0.05 to 0.2 instead of 0.5

**4. Replacing vs Accumulating**:
- Start with accumulating (standard)
- Switch to replacing if states are revisited often
- Replacing is more stable with function approximation

## Common Pitfalls

1. **Forgetting to reset traces**: Must reset e at episode start (or decay for continuing)
2. **Wrong decay**: Use Œ≥Œª, not Œª alone
3. **Trace explosion**: Traces can become very large; consider capping
4. **Memory issues**: For large state spaces, use sparse trace storage
5. **Update order**: Update traces before using them in Q update
6. **Terminal states**: Properly handle traces at episode termination

## Connection to Modern Deep RL

n-step methods and eligibility traces are foundational for:

**n-step Methods**:
- **n-step DQN**: Multi-step TD for deep Q-learning
- **Retrace**: Off-policy n-step with importance sampling
- **IMPALA**: Distributed n-step learning

**Eligibility Traces**:
- **A3C**: Uses n-step or eligibility traces for advantage estimation
- **PPO**: Multi-step returns for policy optimization
- **SAC**: n-step backups in soft actor-critic
- **Rainbow DQN**: Combines n-step with other improvements

Understanding these methods deeply is essential for modern RL research and applications.

## Questions to Consider

1. Why is there often an optimal n between 1 and ‚àû?
2. How does Œª in TD(Œª) relate to n in n-step TD?
3. Why are eligibility traces more memory efficient than storing n-step history?
4. When would you prefer accumulating vs replacing traces?
5. How do eligibility traces help with sparse rewards?
6. Can you derive the TD(Œª) backward view from the forward view?

## Next Steps

After mastering n-step methods and eligibility traces:
- **Week 7**: Planning and Learning (Dyna, MCTS)
- Understanding model-based vs model-free integration
- Combining learning from real experience with simulated experience
- Preparation for advanced topics like policy gradients and function approximation

This week bridges tabular TD methods to more sophisticated algorithms that balance bias and variance for efficient learning!
